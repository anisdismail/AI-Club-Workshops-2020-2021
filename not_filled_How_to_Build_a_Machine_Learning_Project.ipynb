{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "not_filled_How to Build a Machine Learning Project",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHjeANu5Gn7f"
      },
      "source": [
        "# How to Build a Machine Learning Project\n",
        "Welcome to our first hands-on workshop! We will learn together how to build a Machine Learning Project from scratch. After this workshop, you will be able to:\n",
        "- Understand the purpose of applying Machine Learning to a specific dataset\n",
        "- **Frame the problem** as a **Classification** or **Regression** task\n",
        "- **Manipulate features** of the data in order to better understand them\n",
        "- Perform **Exploratory Data Analysis (EDA)** and extract meaningful insights\n",
        "- **Preprocess data** to be used in Machine Learning algorithms\n",
        "- Choose the **best algorithm** that maximizes a performance metric of choice\n",
        "- Choose the **best hyperparameters** that maximize model performance\n",
        "- **Evaluate** the **performance** of the model on **test data**\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqa1t16GcE6x"
      },
      "source": [
        "Add meme or cute gif"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KyX502yaUak"
      },
      "source": [
        "First, let's import the libraries that we will need for this project: <br>\n",
        "- [Numpy](https://numpy.org/doc/) - for matrices and vectors manipulation and operations \n",
        "- [Pandas](https://pandas.pydata.org/docs/getting_started/overview.html) - for data cleaning and analysis\n",
        "- [Scikit-Learn](https://scikit-learn.org/stable/) - for building predictive models\n",
        "- [Matplotlib](https://matplotlib.org/) and [Seaborn](https://seaborn.pydata.org/) - for visualizing various types of plots\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoRXCb2-hdUc"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "RANDOM_SEED=42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "pd.options.mode.chained_assignment = None "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_dl2-TJNIJn"
      },
      "source": [
        "## **0. Understand the Problem**\n",
        "The very first step is **understand the given problem** and **identify the objective** of the task. By finding the purpose of the task, we can make sure whether the problem can be framed as a Machine Learning Problem or not.<br>\n",
        "\n",
        "---\n",
        "\n",
        "For the sake of this workshop, the objective of the task is already defined:<br>\n",
        "###\"Given various socio-economic features, build a model to predict housing prices in California using the California census data..\"<br>\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeNKZ_XiMxCw"
      },
      "source": [
        "## **1. Get the Data**\n",
        "The first step is to download the dataset and load it into Colab runtime. We will use the California census dataset published on [Statslib]():\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2prC3EWckNh"
      },
      "source": [
        "import os\n",
        "import tarfile\n",
        "from six.moves import urllib\n",
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
        "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
        "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
        "\n",
        "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
        " if not os.path.isdir(housing_path):\n",
        "    os.makedirs(housing_path)\n",
        " tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
        " urllib.request.urlretrieve(housing_url, tgz_path)\n",
        " housing_tgz = tarfile.open(tgz_path)\n",
        " housing_tgz.extractall(path=housing_path)\n",
        " housing_tgz.close()\n",
        "\n",
        "fetch_housing_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeq-kDAGeSl5"
      },
      "source": [
        "Let's read the dataset into a Dataframe, a special data structure used in Pandas with various helpful functions for data analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1sJKgny7DCS"
      },
      "source": [
        "data_path=os.path.join(HOUSING_PATH,'housing.csv')\n",
        "df=pd._______(data_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsv_qSH8hbBv"
      },
      "source": [
        "Let's check the dimensions of the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwlenkP2hu_e"
      },
      "source": [
        "____,____=df.____\n",
        "print(\"Number of samples:\",_____,\"\\nNumber of features:\",_______)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQOeYHH_vjfr"
      },
      "source": [
        "Let us take a look on the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r2Y9WKDve_r"
      },
      "source": [
        "df.____()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBCV7rl-v31B"
      },
      "source": [
        "Each row represents one district and there are 10 attributes: \n",
        "- longitude \n",
        "- latitude\n",
        "- housing_median_age\n",
        "- total_rooms\n",
        "- total_bedrooms\n",
        "- population\n",
        "-households\n",
        "- median_income\n",
        "- ocean_proximity\n",
        "-**median_house_value (target feature)**\n",
        "\n",
        "Since we want to predict the **median house value** feature, which is a **continuous variable**, we can frame the problem as a **regression problem**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVWhRl2gmWVP"
      },
      "source": [
        "Next, let's check some useful informations about the features count and their data types:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeXpCES_mg99"
      },
      "source": [
        "df.____()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATkfQHRhl7fU"
      },
      "source": [
        "We notice that:\n",
        "- There are two different data types: **float64** and **Object** data types (String data type in this case)\n",
        "- total_bedrooms features has some missing values that we need to do something about later on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11-m-DAFkbPx"
      },
      "source": [
        "A very important step before examining the data is to split the data into **train and test data** to avoid **data snooping**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzdaaXUhl-a0"
      },
      "source": [
        "from ____.__________ import _______\n",
        "train_df, test_df=______(______,test_size=_____,random_state=_____)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlSmG4YAbuF_"
      },
      "source": [
        "## **2. Explore the Data** \n",
        "Next, let's try to go deeper in understanding the features of the dataset:<br>\n",
        "First, let's start by getting some useful statistics about the features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rRlw9iucD2S"
      },
      "source": [
        "train_df._______"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOU6IH8TcEY3"
      },
      "source": [
        "To verify our observations, let's visualize these features distributions: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi9esPHFpkuu"
      },
      "source": [
        "fig = plt.figure(figsize = (15,20))\n",
        "ax = fig.gca()\n",
        "train_df.___(_____)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXl4MJHoqw-U"
      },
      "source": [
        "The vizualizations confirmed our suspicions:\n",
        "- Each feature has its range which makes the learning later on quite slow, which mean that we need to rescale all features to accelerate learning.\n",
        "- All of total_rooms, total_bedroom, population and household features are highly skewed to one side, which mean that there are outliers that we need to take care of.\n",
        "-  The target feature distribution is similar to a gaussian distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGGEFwbitnUE"
      },
      "source": [
        "Next, let's investigate the relationship between the location and median house value. <br>\n",
        "Let's first plot the longtitude vs the latitude:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIS3t-A3t_SO"
      },
      "source": [
        "train_df.plot(kind=_____,x=___,y=___,alpha=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeWfLknHuhPV"
      },
      "source": [
        "To make the map of california more informative for our task, let us add the median house value to the plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyM-x9wmvKKg"
      },
      "source": [
        "train_df.plot(kind=_____,x=_____,y=______,alpha=0.1,c=__________, cmap=plt.get_cmap(\"jet\"),colorbar=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSmeq0MB3iEY"
      },
      "source": [
        "We notice that the closer the districts are the ocean, the higher the value of the median house value, which highlights the importance of the longtitude and latitude features.\n",
        "<br>\n",
        "\n",
        "Let's investigate now the categorical feature \"Ocean proximity\":\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ9O3Q264kI-"
      },
      "source": [
        "plt._____(train_df[________])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_ppyiVY9sVB"
      },
      "source": [
        "We notice that there are 5 unique classes with the maximum class is <1H OCEAN and the minimum class is ISLAND.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpz0yPPJ-cys"
      },
      "source": [
        "Next, let's invetigate the correlations between the features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rir9COF-jks"
      },
      "source": [
        "corr=train_df._____\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "sns.heatmap(______, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q2ePfKUDnTK"
      },
      "source": [
        "We notice that:\n",
        "- there is a strong correlation between the target feature and the spatial location (longitude and latitude), confirming our previous observations.\n",
        "- there is also a strong correlation between the target feature and median income\n",
        "- note that there is a strong correlation between households and total_rooms, population and households, and total_rooms and nb_of_bedrooms. We might use these correlations to craft new features later on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xr8HVjVSN7_"
      },
      "source": [
        "Let's find the correlation between these newly crafted features and the target value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0_Kdm4-SygZ"
      },
      "source": [
        "train_df[\"rooms_per_household\"]=train_df[__________]/train_df[__________]\n",
        "train_df[\"bedrooms_per_total\"]=train_df[___________]/train_df[___________]\n",
        "train_df[\"households_per_population\"]=train_df[________]/train_df[__________]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_fct2Tfji-L"
      },
      "source": [
        "corr=train_df._____\n",
        "corr[___________].___________"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaUcEnBotRtT"
      },
      "source": [
        "Neat! We notice that bedrooms_per_total, rooms_per_household, and households_per_population have higher correlation with the target attribute than the original features. This means that these features would be a great addition to our training features.\n",
        "<br>\n",
        "Let's remove them for the moment from the training features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BazySHrSh2c6"
      },
      "source": [
        "train_df=train_df.drop([\"rooms_per_household\",\"bedrooms_per_total\",\"households_per_population\"],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msHp1KoAh3er"
      },
      "source": [
        "## **3. Prepare the Data**\n",
        "\n",
        "After analyzing the data, let's recap the steps we need to do:\n",
        "- Fill the missing values in the total_bedrooms feature\n",
        "- Encode the Ocean Proximity feature into an integer value\n",
        "- Standardize all the numerical features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8CyEH0xRcre"
      },
      "source": [
        "y_train_df=train_df[___________].copy()\n",
        "x_train_df=train_df.drop(_________,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR-4p0-S85tn"
      },
      "source": [
        "### Dealing with Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BgCXJ9NtQCa"
      },
      "source": [
        "from ___________ import __________\n",
        "impute=________(strategy=_____)\n",
        "train_df_num=x_train_df.drop(\"ocean_proximity\",axis=1)\n",
        "impute._____(________)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG-QZ8Yq-Sb4"
      },
      "source": [
        "Let's make sure the imputer has really fitted the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfUAfFnA-Plw"
      },
      "source": [
        "impute.statistics_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCP-ovZl-DJD"
      },
      "source": [
        "train_df_num.median()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8Spy1t1-vCE"
      },
      "source": [
        "Both median values match! Let's fill the missing values now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b26FhL0eC0El"
      },
      "source": [
        "impute._________(__________)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGli1DzNDfvj"
      },
      "source": [
        "train_df_num.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjkkZLpdDnNG"
      },
      "source": [
        "Great, we made sure all missing values are filled. <br>\n",
        "### Dealing with Categorical values\n",
        "Next, let's transform the \"Ocean proximit\" feature into a numerical feature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqkhBB_3IiIj"
      },
      "source": [
        "train_cat_df=train_df[\"ocean_proximity\"].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzRNUjkkDwc-"
      },
      "source": [
        "from _____________ import ___________\n",
        "ohe=___________(sparse=False)\n",
        "ohe.fit(_________.values.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ6QYKNnHd7C"
      },
      "source": [
        "train_cat_ohe=ohe._________(_________.values.reshape(-1,1))\n",
        "train_cat_ohe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ2K2ophKIQR"
      },
      "source": [
        "### Scaling features\n",
        "As we discusssed before, let's standardize all numerical features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SThKB1MfJ8xB"
      },
      "source": [
        "from _____________ import ___________\n",
        "std_scaler=__________()\n",
        "train_df_num_scaled=std_scaler.___________(________)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VbRNvqvNvNx"
      },
      "source": [
        "### Creating the Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYg51VwWNpOG"
      },
      "source": [
        "Now that we did all the previous tasks, let's prepare the pipeline for our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GOlfJivN4mO"
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
        "\n",
        "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self): \n",
        "    pass\n",
        "  def fit(self, X, y=None):\n",
        "    return self # nothing else to do\n",
        "  \n",
        "  def transform(self, X, y=None):\n",
        "    rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
        "    population_per_household = X[:, population_ix] / X[:, households_ix]\n",
        "    bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
        "    return np.c_[X, rooms_per_household, population_per_household,bedrooms_per_room]\n",
        "\n",
        "attrib_adder = CombinedAttributesAdder()\n",
        "train_extra_attribs = attrib_adder.transform(x_train_df.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dek3R60YP0UN"
      },
      "source": [
        "from __________ import ________\n",
        "num_pipeline=_______([(\"imputer\",_______),\n",
        "                        (\"extra_attribs\",_______________),\n",
        "                      (\"std_scaler\",_______________),\n",
        "                      ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo-mHB1ePYpe"
      },
      "source": [
        "Let's combine the num_pipeline and the catergorical pipeline into one pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUXrgxE6Qhas"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "num_attributes=___________.________\n",
        "cat_attributes=[______________]\n",
        "\n",
        "full_pipeline=ColumnTransformer([\n",
        "                                 (\"num\",__________,num_attributes),\n",
        "                                 (\"cat\",__________,cat_attributes),\n",
        "                              ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xptD0VClUHkU"
      },
      "source": [
        "Now, we can preprocess the original dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "badKdzaJUG4F"
      },
      "source": [
        "x_train_prepared=full_pipeline.________________(______________)\n",
        "y_train=__________.___________"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIdmjw8AU67T"
      },
      "source": [
        "## **5. Short-List Promising Models**\n",
        "Now that we have prepared our data, it is time to train the machine learning models!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbEJKlNkV0cM"
      },
      "source": [
        "from ______________ import _______________\n",
        "lin_model=_____________()\n",
        "lin_model.___(_______,_________)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36Sxo-Q6XDd9"
      },
      "source": [
        "Let's evaluate some of the predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x-Sw0lkXCza"
      },
      "source": [
        "labels=y_train[:5]\n",
        "sampled=x_train_prepared[:5,:]\n",
        "print(\"Labels:\",labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koXfEqlPXiyQ"
      },
      "source": [
        "pred_labels=________.________(_________)\n",
        "print(\"Predicted labels:\",pred_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UphpYzqVX7_P"
      },
      "source": [
        "Not bad at all! However the predictedvalues are not very close to the actual values. We need some kind of performance metric to measure how close we are to the actual values.<br>\n",
        "One useful metric in regression is the Root Mean Square Error (RMSE):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jsc_Q7bpYa5z"
      },
      "source": [
        "from ___________ import ___________\n",
        "pred_labels=________._______(__________)\n",
        "mse=____________(__________,__________)\n",
        "print(\"RMSE:\",np.sqrt(mse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5vtx__BZHH2"
      },
      "source": [
        "Okay this score is not that good! With this simple model, we can infer that the model is underfitting. Let's try a more powerful model like Decision Trees:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFcyWxuiZj6E"
      },
      "source": [
        "from __________ import ________\n",
        "dtr_model=______________\n",
        "dtr_model.____(______,________)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1MuN3JLZ5UW"
      },
      "source": [
        "from ___________ import ___________\n",
        "pred_labels=________._______(__________)\n",
        "mse=____________(__________,__________)\n",
        "print(\"RMSE:\",np.sqrt(mse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n6ah71raUGm"
      },
      "source": [
        "Clearly there is a problem, is this model perfect? Let's us double check with a more robust evaluation technique: cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyJxhjunamIV"
      },
      "source": [
        "from ___________ import __________\n",
        "scores=___________(dtr_model,x_train_prepared,y_train,cv=_____,scoring=________)\n",
        "scores=np.sqrt(-scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs3UsnkUbNGF"
      },
      "source": [
        "print(\"mean rmse:\",np.mean(scores))\n",
        "print(\"std:\",np.std(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBLIW3PebgSj"
      },
      "source": [
        "Now we know that this model is a bit worse than the linear model, there is a possibility that the model is overfitting the data. Let's us now a better model like Random Forests. Usually such models called Ensemble models perform better than simpler models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NygbQf-b0nh"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rand_model=RandomForestRegressor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSwy4bjAb3r6"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores=cross_val_score(rand_model,x_train_prepared,y_train,cv=5,scoring=\"neg_mean_squared_error\")\n",
        "scores=np.sqrt(-scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-WQx8w1b63V",
        "outputId": "efd5108b-f8ad-4d7d-a40f-351d0ae8a702",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"mean rmse:\",np.mean(scores))\n",
        "print(\"std:\",np.std(scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean rmse: 50447.002014296806\n",
            "std: 702.8995519311321\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJSzO0NHdszk"
      },
      "source": [
        "Wow! The model has improved drastically!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7C-D0_WfSf4"
      },
      "source": [
        "## **6. Fine Tune the System and Test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDXQaIwnfHKa"
      },
      "source": [
        "Usually after this step follows the hyperparameters tuning stage, where we try tuning our model hyperparameters to achieve higher results.\n",
        "For more information, check the [GridSearch](https://scikit-learn.org/stable/modules/grid_search.html) approach in Scikit Learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veeJ34p5jqG3"
      },
      "source": [
        "from ____________ import _____________\n",
        "param_grid = [\n",
        " {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
        " {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
        " ]\n",
        "\n",
        "forest_reg = ___________()\n",
        "grid_search = __________(forest_reg, param_grid, cv=5,\n",
        " scoring='neg_mean_squared_error',\n",
        "return_train_score=True)\n",
        "\n",
        "grid_search.fit(____________,_________)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaDXLbtKkZaW"
      },
      "source": [
        "cvres = grid_search.cv_results_\n",
        "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
        "  print(np.sqrt(-mean_score), params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fos2qMOkG_a"
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQK2j6n8flpj"
      },
      "source": [
        "Now it is time to test our model! We have seen previously that Random Forests perform the best in comparison to the previous models, so we will use it for our test data evaluation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnVaJpv2fKMj"
      },
      "source": [
        "final_model=grid_search.best_estimator_\n",
        "\n",
        "X_test = test_df.drop(\"median_house_value\", axis=1)\n",
        "y_test = test_df[\"median_house_value\"].copy()\n",
        "X_test_prepared = full_pipeline.__________(________)\n",
        "final_predictions = final_model.________(__________)\n",
        "final_mse = _________(y_test, final_predictions)\n",
        "final_rmse = np.sqrt(final_mse) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJe3FK7YjTNV"
      },
      "source": [
        "print(\"final rmse:\",final_rmse)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}