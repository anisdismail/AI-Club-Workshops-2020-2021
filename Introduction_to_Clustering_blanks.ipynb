{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction_to_Clustering_blanks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2z3raGvIW3w"
      },
      "source": [
        "In this notebook, we are going to learn more about **Clustering Algorithms** and mainly **K-Means Algorithm**. We will be learning about:\n",
        "- Training K-Means Algorithm\n",
        "- Evaluating the quality of the clusters generated\n",
        "- Introducing the **Elbow Method** and **Silhouette Method** for selecting the best number of clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZnKce73JAhZ"
      },
      "source": [
        "# K-means Clustering\n",
        "For this task, we will be starting with a dummy dataset for easier visualization and manipulation. <br>\n",
        "Let's start by importing the needed libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXUozkfuF01F"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISHfFrmcJn_x"
      },
      "source": [
        "In the first example, we will generating a dummy dataset of 2000 instances that belong nicely to K clusters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOO29UzjJmye"
      },
      "source": [
        "n_samples = 2000\n",
        "random_state = 48\n",
        "X, y = make_blobs(n_samples=n_samples, random_state=random_state)\n",
        "print(\"number of features:\",X.______)\n",
        "print(\"number of instances:\",X._____)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTlg7_Z3KasW"
      },
      "source": [
        "Let's go ahead and visualize the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21DBzwPkKZ8n"
      },
      "source": [
        "plt.scatter(X[:,0],X[:,1])\n",
        "plt.title(\"Visualizing the Dummy Dataset\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjQwE_t7LVr7"
      },
      "source": [
        "It is very clear that the data can be grouped into 3 clusters in this case. <br> Unfortunately not all the datasets can be easily visualized in 2 dimensions. Later on today we will be introducing an alternative way to tackle datasets with more than 2 features.\n",
        "<br>\n",
        "Next, let's try to use K-Means to detect these clusters: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFxt7fAfJ48U"
      },
      "source": [
        "kmeans = KMeans(n_clusters=___, random_state=random_state)\n",
        "y_pred=kmeans._________(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
        "plt.title(\"Clustered Dataset\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5CddRKHNdHB"
      },
      "source": [
        "Neat! It seems that K-means was easily able to find the 3 clusters! Let's evaluate the performance of K-means:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz-sif6RNthn"
      },
      "source": [
        "print(\"within-cluster sum-of-squares:\",kmeans.__________)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmbFDhUlOGHN"
      },
      "source": [
        "As you might have noticed, inertia is not a very meaningful metric in our case. It is does not tell us how good the clusters are. However, it is really helpful when we need to search for **the optimal number of the clusters** as we will see soon.<br>\n",
        "Next, let's try with **silhouette score**, which measures how similar a point is to its own cluster compared to other clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0tHqUrUO5gb"
      },
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "silhouette_score(_,___, metric = _______)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9gcksdGPePF"
      },
      "source": [
        "That is really good! Knowing that the maximum score for silhouette is 1, this shows that elements inside the clusters are similar to each other while being dissimilar to points in other clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5JvMmszP3D9"
      },
      "source": [
        "Next, let's investigate a real dataset and try to get some insights about the structure of the data. We will be using the Iris dataset which is usally a classification dataset for flower types based on their sepal and petal measures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sDXUIvfPdVQ"
      },
      "source": [
        "from sklearn import datasets\n",
        "iris=datasets.load_iris()\n",
        "print(iris.DESCR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BBs5bm1Rnua"
      },
      "source": [
        "import pandas as pd\n",
        "iris_df=pd.DataFrame(iris.data,columns=iris.feature_names)\n",
        "iris_df[\"species\"]=iris.target\n",
        "iris_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE2k-eidVPzw"
      },
      "source": [
        "print(\"Target classes:\",np.unique(iris.target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmwqIMFdRvtP"
      },
      "source": [
        "As you can see now, we have 4 features which will make visualizing the data a bit hard. There are many approaches we can use like **dimensionality reduction** algorithms, **Andrew plots**... <br>\n",
        "However, in this case we will use a different approach to tackle this clustering problem. We will be using the **Elbow Method**:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOyWBMgLkajp"
      },
      "source": [
        "ine = []\n",
        "kmax = 10\n",
        "\n",
        "for k in range(2, kmax+1):\n",
        "  kmeans = KMeans(n_clusters = k).___(iris.data)\n",
        "  ine.append(kmeans.________)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaZMnwYqknN8"
      },
      "source": [
        "plt.plot(np.arange(2,11),ine)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiyR07gSl9BQ"
      },
      "source": [
        "It seems that using the Elbow method, the best K value is either 3 or 4, which is close to the number of target classes already defined. <br>\n",
        "Next, let's try to use the **Silhouette Method**: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr7Xv9w2UUn_"
      },
      "source": [
        "sil = []\n",
        "kmax = 10\n",
        "\n",
        "for k in range(2, kmax+1):\n",
        "  kmeans = KMeans(n_clusters = k).fit(iris.data)\n",
        "  labels = kmeans.labels_\n",
        "  sil.append(silhouette_score(iris.data, labels, metric = 'euclidean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvhHiqVrUc05"
      },
      "source": [
        "plt.plot(np.arange(2,11),sil)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrKXuTIsVcEY"
      },
      "source": [
        "That is very intruiguing! We already know that there are three classes in this dataset, but why silhouette score is giving the best score for the case of two clusters? <br>\n",
        "To understand what is happening here, we need to go back and investigate the data structure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANUXW0LpV5A_"
      },
      "source": [
        "scatter=plt.scatter(iris_df[\"sepal length (cm)\"],iris_df[\"sepal width (cm)\"],c=iris_df[\"species\"],)\n",
        "plt.legend(handles=scatter.legend_elements()[0],labels=(\"Iris-Setosa\",\"Iris-Versicolour\",\"Iris-Virginica\"))\n",
        "plt.show()\n",
        "\n",
        "scatter=plt.scatter(iris_df[\"petal length (cm)\"],iris_df[\"petal width (cm)\"],c=iris_df[\"species\"],)\n",
        "plt.legend(handles=scatter.legend_elements()[0],labels=(\"Iris-Setosa\",\"Iris-Versicolour\",\"Iris-Virginica\"))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlIXgtb-Xm16"
      },
      "source": [
        "We notice two important points:\n",
        "- Visualizing the data shows that the dataset (at least in its parts) does not have condensed blobs or clusters.\n",
        "- The dataset have 4 features. Knowing that the Euclidian distance is used by K-Means, it will start to lose its meaning as we increase the dimensions. This can prevent the clustering process from converging to the best solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzjXbq8V-esq"
      },
      "source": [
        "## Further Steps\n",
        "Now that you know how to use K-Means, you can try the following:\n",
        "- Use different approaches of clustering, including Hierarchical Clustering\n",
        "- Evaluate the clusters quality with other metrics\n",
        "- Experiment with different datasets"
      ]
    }
  ]
}